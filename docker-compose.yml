version: "3"

networks:
  rag_network:

services:

  ollama:
    image: ollama/ollama:latest
    restart: always
    container_name: rag_ollama
    tty: true
    networks:
    - rag_network
    # entrypoint: ["/bin/sh", "-c"]handlers.auth_handler import *
    # command: "ollama pull deepseek-r1 && ollama serve"
    # # command: sh -c "ollama pull deepseek-r1 && ollama serve"
    ports:
      - 11434:11434
    expose:
      - 11434
    volumes:
      - ./init-ollama.sh:/init-ollama.sh
      - ./rag_ollama:/root/.ollama
      - ./ollama_models:/root/.ollama/models
    entrypoint: ["/init-ollama.sh"]
    environment:
      - OLLAMA_NUM_PARALLEL=2
      - OLLAMA_MAX_LOADED_MODELS=2
      - OLLAMA_KEEP_ALIVE=24h
      - OLLAMA_HOST=0.0.0.0
    deploy:
      resources:
        reservations:
          devices:
          - driver: nvidia
            capabilities: [ gpu ]

  qdrant:
    image: qdrant/qdrant:latest
    restart: always
    container_name: rag_qdrant
    networks:
    - rag_network
    ports:
      - 6333:6333
      - 6334:6334
    expose:
      - 6333
      - 6334
    volumes:
      - ./qdrant_data:/qdrant/storage

  postgres:
    image: postgres:latest
    container_name: rag_postgres
    networks:
      - rag_network
    restart: always
    volumes:
      - postgres_data:/var/lib/postgresql/data/
    ports:
      - 5050:5432
    environment:
      - POSTGRES_DB=rag_db
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=postgres

  backend:
    build: . 
    container_name: rag_api
    networks:
      - rag_network
    restart: on-failure
    command: bash -c "alembic upgrade head && python3 -m src.main"
    ports:
      - 8000:8000
    volumes:
      - ./cache:/app/cache:cached
      - .:/app
    depends_on:
      - postgres
      - qdrant
      - ollama
    env_file: .env
    environment:
      - PG_HOST=postgres
      - FASTEMBED_CACHE_DIR=/app/cache
      - HF_HOME=/app/cache

volumes:
  postgres_data: